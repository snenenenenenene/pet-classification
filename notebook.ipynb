{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pet Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "It happens more often than not - especially to animal & pet lovers such as myself - that you come across the most beautiful dog while going for a walk in the park -- or whenever you're mindlessly scrolling Facebook for hours. This project is a digitalised manifestation of your gung ho dog breed connoisseur from around the corner. How? Through the means of **Machine Learning** (ML).\n",
    "The vast majority of blood, sweat and tears spent on researching and implementing the trained model will be documented by the use of the **Jupyter Notebook** you're currently reading.\n",
    "Nevertheless, to take into account the target audience and pragmatism of this implementation, I have chosen to pair the aforementioned notebook with a frontend developed in React in order to evade the convoluted process of having to manually import images into the model -- which is still an option if you're into self-loathing.\n",
    "\n",
    "The dataset used in this project is the [Animal Breed - Cats and Dogs](https://www.kaggle.com/imsparsh/animal-breed-cats-and-dogs) dataset from Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your usual myriad of ML libraries were imported in this project such as:\n",
    "- Numpy (Arrays & matrices) \n",
    "- Pandas (Manage the arrays)\n",
    "- OS (Communication between your filesystem and python)\n",
    "- Matplotlib (Plotting graphs)\n",
    "- Tensorflow & Keras (Machine Learning)\n",
    "\n",
    "Other than these I have also imported some lesser known libraries, and modules especially, to adapt to the specific circumstances of this project. These being:\n",
    "- Preprocessing.image (Loading in images and transforming them into tensors)\n",
    "- Sklearn.metrics (Visualising & reporting the model's performance & efficiency)\n",
    "- Tensorflowjs (Exporting to a Javascript compatible model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import classification_report, log_loss, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflowjs as tfjs\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to adhere to coding conventions and to avoid confusion further on in the project I have tactically split up the setup part of my code from the rest in order to declare some constants and to initialise the mapping as well as the reverse mapping which will we'll take under the loop a bit farther down the project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = 'breeds/TRAIN'\n",
    "TEST = 'breeds/TEST'\n",
    "IMAGE_HEIGHT = 256\n",
    "IMAGE_WIDTH = 256\n",
    "EPOCH_CNT = 3\n",
    "\n",
    "dataset=[] \n",
    "name=[]\n",
    "count=0\n",
    "pet_dict=[]\n",
    "\n",
    "Name = os.listdir(TRAIN)\n",
    "N=list(range(len(Name)))\n",
    "mapping = dict(zip(Name, N))\n",
    "reverse_mapping = dict(zip(N, Name))\n",
    "\n",
    "print(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning & Processing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this project is almost entirely image based there quite simply isn't a lot of cleaning, imputation or processing to be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for file in sorted(os.listdir(TRAIN)):\n",
    "    path=os.path.join(TRAIN,file)\n",
    "    for im in os.listdir(path):\n",
    "        image=load_img(os.path.join(path,im), grayscale=False, color_mode='rgb', target_size=(IMAGE_HEIGHT,IMAGE_WIDTH))\n",
    "        image=img_to_array(image)\n",
    "        image=image/255.0\n",
    "        dataset.append([image,count])     \n",
    "    count=count+1\n",
    "test=[]\n",
    "testfile=[]\n",
    "\n",
    "for file in os.listdir(TEST):\n",
    "    path=os.path.join(TEST,file)\n",
    "    image=load_img(path, grayscale=False, color_mode='rgb', target_size=(IMAGE_HEIGHT,IMAGE_WIDTH))\n",
    "    image=img_to_array(image)\n",
    "    image=image/255.0\n",
    "    test+=[image]\n",
    "    testfile+=[file]\n",
    "data,labels0=zip(*dataset)\n",
    "labels1=to_categorical(labels0)\n",
    "labels=np.array(labels1)\n",
    "data=np.array(data)\n",
    "test=np.array(test)\n",
    "data2=data.reshape(-1,IMAGE_HEIGHT,IMAGE_WIDTH,3)\n",
    "test2=test.reshape(-1,IMAGE_HEIGHT,IMAGE_WIDTH,3)\n",
    "trainx,testx,trainy,testy=train_test_split(data,labels,test_size=0.2,random_state=44)\n",
    "print(trainx.shape)\n",
    "print(testx.shape)\n",
    "print(trainy.shape)\n",
    "print(testy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to actually train the algorithm we'll make use of [**Densely Connected Convolutional Networks**](https://arxiv.org/abs/1608.06993) (DCCN). To go about this in a comprehensive and easy to use manner, we'll be implementing said DCCN using a Keras module which we've imported in one of the former stages of this document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(horizontal_flip=True, vertical_flip=True, rotation_range=20, zoom_range=0.2,\n",
    "                             width_shift_range=0.2, height_shift_range=0.2, shear_range=0.1, fill_mode=\"nearest\")\n",
    "\n",
    "pretrained_model = tf.keras.applications.DenseNet121(input_shape=(\n",
    "    IMAGE_HEIGHT, IMAGE_WIDTH, 3), include_top=False, weights='imagenet', pooling='avg')\n",
    "\n",
    "pretrained_model.trainable = False\n",
    "\n",
    "inputs = pretrained_model.input\n",
    "x3 = tf.keras.layers.Dense(128, activation='relu')(pretrained_model.output)\n",
    "outputs = tf.keras.layers.Dense(37, activation='softmax')(x3)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "his = model.fit(datagen.flow(trainx, trainy, batch_size=32),\n",
    "                validation_data=(testx, testy), epochs=EPOCH_CNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fitness\n",
    "\n",
    "In order to fathom the efficiency and as to how our newly born model performs we will be using a couple of built-in modules and functions from the Sklearn package. \n",
    "Previously the trained model is stored in the \"his\" variable. After going through the various learning cycles its accuracy and loss data are then stored in sepperate new variables and are then plotted in the next section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(testx)\n",
    "pred = np.argmax(y_pred, axis=1)\n",
    "ground = np.argmax(testy, axis=1)\n",
    "print(classification_report(ground, pred))\n",
    "\n",
    "get_acc = his.history['accuracy']\n",
    "value_acc = his.history['val_accuracy']\n",
    "get_loss = his.history['loss']\n",
    "validation_loss = his.history['val_loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(len(get_acc))\n",
    "plt.plot(epochs, get_acc, 'r', label='Accuracy of Training data')\n",
    "plt.plot(epochs, value_acc, 'b', label='Accuracy of Validation data')\n",
    "plt.title('Training VS Validation Accuracy')\n",
    "plt.legend(loc=0)\n",
    "plt.figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(len(get_loss))\n",
    "plt.plot(epochs, get_loss, 'r', label='Loss of Training data')\n",
    "plt.plot(epochs, validation_loss, 'b', label='Loss of Validation data')\n",
    "plt.title('Training VS Validation Loss')\n",
    "plt.legend(loc=0)\n",
    "plt.figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proof testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is quite possibly the most gratifying part of this entire project. Sometimes at least. \n",
    "Firstly, we declare the reverse mapping of breeds (see #Setup) to then map the different representations of breeds - using integers - to their respective indeces. A random, yet cherry-picked for the sake of **proof** testing, is then loaded in and passed to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image=load_img(\"breeds/TEST/beagle/101.jpg\",target_size=(256,256))\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "model = keras.models.load_model('models/model1.h5')\n",
    "\n",
    "image=img_to_array(image) \n",
    "image=image/255.0\n",
    "prediction_image=np.array(image)\n",
    "prediction_image= np.expand_dims(image, axis=0)\n",
    "\n",
    "def mapper(value):\n",
    "    return reverse_mapping[value]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICTION\n",
    "# print(prediction_image)\n",
    "prediction=model.predict(prediction_image)\n",
    "value=np.argmax(prediction)\n",
    "move_name=mapper(value)\n",
    "print(f\"Prediction is {format(move_name).capitalize()}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make use of the functionality of the model it will be serialised as well as converted into a model which is able to be interpreted by the aforementioned React app. The TensorflowJS converter module will split up the model into one model.json as well as various weight files which will add their respective weights to the breeds. After this is done and dusted it is ready to be pushed straight to github in order for the raw model.json file to receive GET-requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment to save model & to export it to a tsjs compatible format.\n",
    "with open('frontend/src/data/mapping.json', 'w') as mapping:\n",
    "    json.dump(reverse_mapping, mapping)\n",
    "\n",
    "model.save('models/model1.h5')\n",
    "tfjs.converters.save_keras_model(model, 'models/tfjs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With an 89% accuracy one might say that this model is pretty spot on. There are still some times where the Keras model predicts something completely different from the selected image yet the instances where this does happen are mainly caused by the fact that the image has bad lighting. This is a big issue since colour is one of the hyperparameters and if the colour is not visible the model has a big problem predicting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
